{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOB1BfEKlknJv5gEvUvdfG5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"6IM_1ZGotqMz","executionInfo":{"status":"ok","timestamp":1759119918959,"user_tz":-330,"elapsed":3187,"user":{"displayName":"Dr. Nagendra","userId":"05632517937859890145"}}},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","source":["df=pd.read_csv('/content/Obesity Classification.csv')\n","df.head(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"bi6SgYJSuA37","executionInfo":{"status":"ok","timestamp":1759119990196,"user_tz":-330,"elapsed":133,"user":{"displayName":"Dr. Nagendra","userId":"05632517937859890145"}},"outputId":"b50446fc-6d91-464b-c7b1-1e78698d6bce"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   ID  Age  Gender  Height  Weight   BMI          Label\n","0   1   25    Male     175      80  25.3  Normal Weight\n","1   2   30  Female     160      60  22.5  Normal Weight\n","2   3   35    Male     180      90  27.3     Overweight\n","3   4   40  Female     150      50  20.0    Underweight\n","4   5   45    Male     190     100  31.2          Obese\n","5   6   50  Female     140      40  16.7    Underweight\n","6   7   55    Male     200     110  34.2          Obese\n","7   8   60  Female     130      30  13.3    Underweight\n","8   9   65    Male     210     120  37.2          Obese\n","9  10   70  Female     120      20  10.0    Underweight"],"text/html":["\n","  <div id=\"df-dbecbd8c-c391-4200-a30a-da567177e103\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>Age</th>\n","      <th>Gender</th>\n","      <th>Height</th>\n","      <th>Weight</th>\n","      <th>BMI</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>25</td>\n","      <td>Male</td>\n","      <td>175</td>\n","      <td>80</td>\n","      <td>25.3</td>\n","      <td>Normal Weight</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>30</td>\n","      <td>Female</td>\n","      <td>160</td>\n","      <td>60</td>\n","      <td>22.5</td>\n","      <td>Normal Weight</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>35</td>\n","      <td>Male</td>\n","      <td>180</td>\n","      <td>90</td>\n","      <td>27.3</td>\n","      <td>Overweight</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>40</td>\n","      <td>Female</td>\n","      <td>150</td>\n","      <td>50</td>\n","      <td>20.0</td>\n","      <td>Underweight</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>45</td>\n","      <td>Male</td>\n","      <td>190</td>\n","      <td>100</td>\n","      <td>31.2</td>\n","      <td>Obese</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>6</td>\n","      <td>50</td>\n","      <td>Female</td>\n","      <td>140</td>\n","      <td>40</td>\n","      <td>16.7</td>\n","      <td>Underweight</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>7</td>\n","      <td>55</td>\n","      <td>Male</td>\n","      <td>200</td>\n","      <td>110</td>\n","      <td>34.2</td>\n","      <td>Obese</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>8</td>\n","      <td>60</td>\n","      <td>Female</td>\n","      <td>130</td>\n","      <td>30</td>\n","      <td>13.3</td>\n","      <td>Underweight</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>9</td>\n","      <td>65</td>\n","      <td>Male</td>\n","      <td>210</td>\n","      <td>120</td>\n","      <td>37.2</td>\n","      <td>Obese</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>10</td>\n","      <td>70</td>\n","      <td>Female</td>\n","      <td>120</td>\n","      <td>20</td>\n","      <td>10.0</td>\n","      <td>Underweight</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dbecbd8c-c391-4200-a30a-da567177e103')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-dbecbd8c-c391-4200-a30a-da567177e103 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-dbecbd8c-c391-4200-a30a-da567177e103');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-70bac7c6-3725-4cd8-a04a-e9a6140d332a\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-70bac7c6-3725-4cd8-a04a-e9a6140d332a')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-70bac7c6-3725-4cd8-a04a-e9a6140d332a button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 108,\n  \"fields\": [\n    {\n      \"column\": \"ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 31,\n        \"min\": 1,\n        \"max\": 110,\n        \"num_unique_values\": 108,\n        \"samples\": [\n          80,\n          11,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 24,\n        \"min\": 11,\n        \"max\": 112,\n        \"num_unique_values\": 75,\n        \"samples\": [\n          45,\n          41,\n          18\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Gender\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Female\",\n          \"Male\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Height\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 27,\n        \"min\": 120,\n        \"max\": 210,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          210,\n          160\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Weight\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28,\n        \"min\": 10,\n        \"max\": 120,\n        \"num_unique_values\": 23,\n        \"samples\": [\n          105,\n          20\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BMI\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.5838181808419725,\n        \"min\": 3.9,\n        \"max\": 37.2,\n        \"num_unique_values\": 25,\n        \"samples\": [\n          37.2,\n          28.9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Overweight\",\n          \"Obese\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["df=df.drop('ID',axis=1)\n","df['Gender']=np.where(df['Gender']=='Male',1,0)\n","y=df['Label']\n","x=df.drop('Label',axis=1)"],"metadata":{"id":"hTcNQAJQuQ_X","executionInfo":{"status":"ok","timestamp":1759120004124,"user_tz":-330,"elapsed":29,"user":{"displayName":"Dr. Nagendra","userId":"05632517937859890145"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=42,train_size=0.70)"],"metadata":{"id":"YLq9RmOZuSv9","executionInfo":{"status":"ok","timestamp":1759120011364,"user_tz":-330,"elapsed":66,"user":{"displayName":"Dr. Nagendra","userId":"05632517937859890145"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["rf=RandomForestClassifier(n_estimators=1,criterion=\"entropy\",min_samples_split=3,random_state=41)\n","model=rf.fit(x_train,y_train)\n","model.score(x_test,y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VRtb0KZfuUHk","executionInfo":{"status":"ok","timestamp":1759120016906,"user_tz":-330,"elapsed":28,"user":{"displayName":"Dr. Nagendra","userId":"05632517937859890145"}},"outputId":"7120e0fd-133b-45da-f03c-21686c781146"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.0"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["help(RandomForestClassifier)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"drIuZByXubwA","executionInfo":{"status":"ok","timestamp":1759120051876,"user_tz":-330,"elapsed":431,"user":{"displayName":"Dr. Nagendra","userId":"05632517937859890145"}},"outputId":"187109c4-6d0d-4448-8933-41d3acb98885"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Help on class RandomForestClassifier in module sklearn.ensemble._forest:\n","\n","class RandomForestClassifier(ForestClassifier)\n"," |  RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n"," |\n"," |  A random forest classifier.\n"," |\n"," |  A random forest is a meta estimator that fits a number of decision tree\n"," |  classifiers on various sub-samples of the dataset and uses averaging to\n"," |  improve the predictive accuracy and control over-fitting.\n"," |  Trees in the forest use the best split strategy, i.e. equivalent to passing\n"," |  `splitter=\"best\"` to the underlying :class:`~sklearn.tree.DecisionTreeClassifier`.\n"," |  The sub-sample size is controlled with the `max_samples` parameter if\n"," |  `bootstrap=True` (default), otherwise the whole dataset is used to build\n"," |  each tree.\n"," |\n"," |  For a comparison between tree-based ensemble models see the example\n"," |  :ref:`sphx_glr_auto_examples_ensemble_plot_forest_hist_grad_boosting_comparison.py`.\n"," |\n"," |  Read more in the :ref:`User Guide <forest>`.\n"," |\n"," |  Parameters\n"," |  ----------\n"," |  n_estimators : int, default=100\n"," |      The number of trees in the forest.\n"," |\n"," |      .. versionchanged:: 0.22\n"," |         The default value of ``n_estimators`` changed from 10 to 100\n"," |         in 0.22.\n"," |\n"," |  criterion : {\"gini\", \"entropy\", \"log_loss\"}, default=\"gini\"\n"," |      The function to measure the quality of a split. Supported criteria are\n"," |      \"gini\" for the Gini impurity and \"log_loss\" and \"entropy\" both for the\n"," |      Shannon information gain, see :ref:`tree_mathematical_formulation`.\n"," |      Note: This parameter is tree-specific.\n"," |\n"," |  max_depth : int, default=None\n"," |      The maximum depth of the tree. If None, then nodes are expanded until\n"," |      all leaves are pure or until all leaves contain less than\n"," |      min_samples_split samples.\n"," |\n"," |  min_samples_split : int or float, default=2\n"," |      The minimum number of samples required to split an internal node:\n"," |\n"," |      - If int, then consider `min_samples_split` as the minimum number.\n"," |      - If float, then `min_samples_split` is a fraction and\n"," |        `ceil(min_samples_split * n_samples)` are the minimum\n"," |        number of samples for each split.\n"," |\n"," |      .. versionchanged:: 0.18\n"," |         Added float values for fractions.\n"," |\n"," |  min_samples_leaf : int or float, default=1\n"," |      The minimum number of samples required to be at a leaf node.\n"," |      A split point at any depth will only be considered if it leaves at\n"," |      least ``min_samples_leaf`` training samples in each of the left and\n"," |      right branches.  This may have the effect of smoothing the model,\n"," |      especially in regression.\n"," |\n"," |      - If int, then consider `min_samples_leaf` as the minimum number.\n"," |      - If float, then `min_samples_leaf` is a fraction and\n"," |        `ceil(min_samples_leaf * n_samples)` are the minimum\n"," |        number of samples for each node.\n"," |\n"," |      .. versionchanged:: 0.18\n"," |         Added float values for fractions.\n"," |\n"," |  min_weight_fraction_leaf : float, default=0.0\n"," |      The minimum weighted fraction of the sum total of weights (of all\n"," |      the input samples) required to be at a leaf node. Samples have\n"," |      equal weight when sample_weight is not provided.\n"," |\n"," |  max_features : {\"sqrt\", \"log2\", None}, int or float, default=\"sqrt\"\n"," |      The number of features to consider when looking for the best split:\n"," |\n"," |      - If int, then consider `max_features` features at each split.\n"," |      - If float, then `max_features` is a fraction and\n"," |        `max(1, int(max_features * n_features_in_))` features are considered at each\n"," |        split.\n"," |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n"," |      - If \"log2\", then `max_features=log2(n_features)`.\n"," |      - If None, then `max_features=n_features`.\n"," |\n"," |      .. versionchanged:: 1.1\n"," |          The default of `max_features` changed from `\"auto\"` to `\"sqrt\"`.\n"," |\n"," |      Note: the search for a split does not stop until at least one\n"," |      valid partition of the node samples is found, even if it requires to\n"," |      effectively inspect more than ``max_features`` features.\n"," |\n"," |  max_leaf_nodes : int, default=None\n"," |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n"," |      Best nodes are defined as relative reduction in impurity.\n"," |      If None then unlimited number of leaf nodes.\n"," |\n"," |  min_impurity_decrease : float, default=0.0\n"," |      A node will be split if this split induces a decrease of the impurity\n"," |      greater than or equal to this value.\n"," |\n"," |      The weighted impurity decrease equation is the following::\n"," |\n"," |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n"," |                              - N_t_L / N_t * left_impurity)\n"," |\n"," |      where ``N`` is the total number of samples, ``N_t`` is the number of\n"," |      samples at the current node, ``N_t_L`` is the number of samples in the\n"," |      left child, and ``N_t_R`` is the number of samples in the right child.\n"," |\n"," |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n"," |      if ``sample_weight`` is passed.\n"," |\n"," |      .. versionadded:: 0.19\n"," |\n"," |  bootstrap : bool, default=True\n"," |      Whether bootstrap samples are used when building trees. If False, the\n"," |      whole dataset is used to build each tree.\n"," |\n"," |  oob_score : bool or callable, default=False\n"," |      Whether to use out-of-bag samples to estimate the generalization score.\n"," |      By default, :func:`~sklearn.metrics.accuracy_score` is used.\n"," |      Provide a callable with signature `metric(y_true, y_pred)` to use a\n"," |      custom metric. Only available if `bootstrap=True`.\n"," |\n"," |  n_jobs : int, default=None\n"," |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n"," |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n"," |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n"," |      context. ``-1`` means using all processors. See :term:`Glossary\n"," |      <n_jobs>` for more details.\n"," |\n"," |  random_state : int, RandomState instance or None, default=None\n"," |      Controls both the randomness of the bootstrapping of the samples used\n"," |      when building trees (if ``bootstrap=True``) and the sampling of the\n"," |      features to consider when looking for the best split at each node\n"," |      (if ``max_features < n_features``).\n"," |      See :term:`Glossary <random_state>` for details.\n"," |\n"," |  verbose : int, default=0\n"," |      Controls the verbosity when fitting and predicting.\n"," |\n"," |  warm_start : bool, default=False\n"," |      When set to ``True``, reuse the solution of the previous call to fit\n"," |      and add more estimators to the ensemble, otherwise, just fit a whole\n"," |      new forest. See :term:`Glossary <warm_start>` and\n"," |      :ref:`tree_ensemble_warm_start` for details.\n"," |\n"," |  class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n"," |      Weights associated with classes in the form ``{class_label: weight}``.\n"," |      If not given, all classes are supposed to have weight one. For\n"," |      multi-output problems, a list of dicts can be provided in the same\n"," |      order as the columns of y.\n"," |\n"," |      Note that for multioutput (including multilabel) weights should be\n"," |      defined for each class of every column in its own dict. For example,\n"," |      for four-class multilabel classification weights should be\n"," |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n"," |      [{1:1}, {2:5}, {3:1}, {4:1}].\n"," |\n"," |      The \"balanced\" mode uses the values of y to automatically adjust\n"," |      weights inversely proportional to class frequencies in the input data\n"," |      as ``n_samples / (n_classes * np.bincount(y))``\n"," |\n"," |      The \"balanced_subsample\" mode is the same as \"balanced\" except that\n"," |      weights are computed based on the bootstrap sample for every tree\n"," |      grown.\n"," |\n"," |      For multi-output, the weights of each column of y will be multiplied.\n"," |\n"," |      Note that these weights will be multiplied with sample_weight (passed\n"," |      through the fit method) if sample_weight is specified.\n"," |\n"," |  ccp_alpha : non-negative float, default=0.0\n"," |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n"," |      subtree with the largest cost complexity that is smaller than\n"," |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n"," |      :ref:`minimal_cost_complexity_pruning` for details. See\n"," |      :ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py`\n"," |      for an example of such pruning.\n"," |\n"," |      .. versionadded:: 0.22\n"," |\n"," |  max_samples : int or float, default=None\n"," |      If bootstrap is True, the number of samples to draw from X\n"," |      to train each base estimator.\n"," |\n"," |      - If None (default), then draw `X.shape[0]` samples.\n"," |      - If int, then draw `max_samples` samples.\n"," |      - If float, then draw `max(round(n_samples * max_samples), 1)` samples. Thus,\n"," |        `max_samples` should be in the interval `(0.0, 1.0]`.\n"," |\n"," |      .. versionadded:: 0.22\n"," |\n"," |  monotonic_cst : array-like of int of shape (n_features), default=None\n"," |      Indicates the monotonicity constraint to enforce on each feature.\n"," |        - 1: monotonic increase\n"," |        - 0: no constraint\n"," |        - -1: monotonic decrease\n"," |\n"," |      If monotonic_cst is None, no constraints are applied.\n"," |\n"," |      Monotonicity constraints are not supported for:\n"," |        - multiclass classifications (i.e. when `n_classes > 2`),\n"," |        - multioutput classifications (i.e. when `n_outputs_ > 1`),\n"," |        - classifications trained on data with missing values.\n"," |\n"," |      The constraints hold over the probability of the positive class.\n"," |\n"," |      Read more in the :ref:`User Guide <monotonic_cst_gbdt>`.\n"," |\n"," |      .. versionadded:: 1.4\n"," |\n"," |  Attributes\n"," |  ----------\n"," |  estimator_ : :class:`~sklearn.tree.DecisionTreeClassifier`\n"," |      The child estimator template used to create the collection of fitted\n"," |      sub-estimators.\n"," |\n"," |      .. versionadded:: 1.2\n"," |         `base_estimator_` was renamed to `estimator_`.\n"," |\n"," |  estimators_ : list of DecisionTreeClassifier\n"," |      The collection of fitted sub-estimators.\n"," |\n"," |  classes_ : ndarray of shape (n_classes,) or a list of such arrays\n"," |      The classes labels (single output problem), or a list of arrays of\n"," |      class labels (multi-output problem).\n"," |\n"," |  n_classes_ : int or list\n"," |      The number of classes (single output problem), or a list containing the\n"," |      number of classes for each output (multi-output problem).\n"," |\n"," |  n_features_in_ : int\n"," |      Number of features seen during :term:`fit`.\n"," |\n"," |      .. versionadded:: 0.24\n"," |\n"," |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n"," |      Names of features seen during :term:`fit`. Defined only when `X`\n"," |      has feature names that are all strings.\n"," |\n"," |      .. versionadded:: 1.0\n"," |\n"," |  n_outputs_ : int\n"," |      The number of outputs when ``fit`` is performed.\n"," |\n"," |  feature_importances_ : ndarray of shape (n_features,)\n"," |      The impurity-based feature importances.\n"," |      The higher, the more important the feature.\n"," |      The importance of a feature is computed as the (normalized)\n"," |      total reduction of the criterion brought by that feature.  It is also\n"," |      known as the Gini importance.\n"," |\n"," |      Warning: impurity-based feature importances can be misleading for\n"," |      high cardinality features (many unique values). See\n"," |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n"," |\n"," |  oob_score_ : float\n"," |      Score of the training dataset obtained using an out-of-bag estimate.\n"," |      This attribute exists only when ``oob_score`` is True.\n"," |\n"," |  oob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)\n"," |      Decision function computed with out-of-bag estimate on the training\n"," |      set. If n_estimators is small it might be possible that a data point\n"," |      was never left out during the bootstrap. In this case,\n"," |      `oob_decision_function_` might contain NaN. This attribute exists\n"," |      only when ``oob_score`` is True.\n"," |\n"," |  estimators_samples_ : list of arrays\n"," |      The subset of drawn samples (i.e., the in-bag samples) for each base\n"," |      estimator. Each subset is defined by an array of the indices selected.\n"," |\n"," |      .. versionadded:: 1.4\n"," |\n"," |  See Also\n"," |  --------\n"," |  sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n"," |  sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized\n"," |      tree classifiers.\n"," |  sklearn.ensemble.HistGradientBoostingClassifier : A Histogram-based Gradient\n"," |      Boosting Classification Tree, very fast for big datasets (n_samples >=\n"," |      10_000).\n"," |\n"," |  Notes\n"," |  -----\n"," |  The default values for the parameters controlling the size of the trees\n"," |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n"," |  unpruned trees which can potentially be very large on some data sets. To\n"," |  reduce memory consumption, the complexity and size of the trees should be\n"," |  controlled by setting those parameter values.\n"," |\n"," |  The features are always randomly permuted at each split. Therefore,\n"," |  the best found split may vary, even with the same training data,\n"," |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n"," |  of the criterion is identical for several splits enumerated during the\n"," |  search of the best split. To obtain a deterministic behaviour during\n"," |  fitting, ``random_state`` has to be fixed.\n"," |\n"," |  References\n"," |  ----------\n"," |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n"," |\n"," |  Examples\n"," |  --------\n"," |  >>> from sklearn.ensemble import RandomForestClassifier\n"," |  >>> from sklearn.datasets import make_classification\n"," |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n"," |  ...                            n_informative=2, n_redundant=0,\n"," |  ...                            random_state=0, shuffle=False)\n"," |  >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n"," |  >>> clf.fit(X, y)\n"," |  RandomForestClassifier(...)\n"," |  >>> print(clf.predict([[0, 0, 0, 0]]))\n"," |  [1]\n"," |\n"," |  Method resolution order:\n"," |      RandomForestClassifier\n"," |      ForestClassifier\n"," |      sklearn.base.ClassifierMixin\n"," |      BaseForest\n"," |      sklearn.base.MultiOutputMixin\n"," |      sklearn.ensemble._base.BaseEnsemble\n"," |      sklearn.base.MetaEstimatorMixin\n"," |      sklearn.base.BaseEstimator\n"," |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin\n"," |      sklearn.utils._metadata_requests._MetadataRequester\n"," |      builtins.object\n"," |\n"," |  Methods defined here:\n"," |\n"," |  __init__(self, n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n"," |      Initialize self.  See help(type(self)) for accurate signature.\n"," |\n"," |  set_fit_request(self: sklearn.ensemble._forest.RandomForestClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._forest.RandomForestClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n"," |      Request metadata passed to the ``fit`` method.\n"," |\n"," |      Note that this method is only relevant if\n"," |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n"," |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n"," |      mechanism works.\n"," |\n"," |      The options for each parameter are:\n"," |\n"," |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n"," |\n"," |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n"," |\n"," |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n"," |\n"," |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n"," |\n"," |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n"," |      existing request. This allows you to change the request for some\n"," |      parameters and not others.\n"," |\n"," |      .. versionadded:: 1.3\n"," |\n"," |      .. note::\n"," |          This method is only relevant if this estimator is used as a\n"," |          sub-estimator of a meta-estimator, e.g. used inside a\n"," |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n"," |\n"," |      Parameters\n"," |      ----------\n"," |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n"," |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n"," |\n"," |      Returns\n"," |      -------\n"," |      self : object\n"," |          The updated object.\n"," |\n"," |  set_score_request(self: sklearn.ensemble._forest.RandomForestClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.ensemble._forest.RandomForestClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n"," |      Request metadata passed to the ``score`` method.\n"," |\n"," |      Note that this method is only relevant if\n"," |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n"," |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n"," |      mechanism works.\n"," |\n"," |      The options for each parameter are:\n"," |\n"," |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n"," |\n"," |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n"," |\n"," |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n"," |\n"," |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n"," |\n"," |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n"," |      existing request. This allows you to change the request for some\n"," |      parameters and not others.\n"," |\n"," |      .. versionadded:: 1.3\n"," |\n"," |      .. note::\n"," |          This method is only relevant if this estimator is used as a\n"," |          sub-estimator of a meta-estimator, e.g. used inside a\n"," |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n"," |\n"," |      Parameters\n"," |      ----------\n"," |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n"," |          Metadata routing for ``sample_weight`` parameter in ``score``.\n"," |\n"," |      Returns\n"," |      -------\n"," |      self : object\n"," |          The updated object.\n"," |\n"," |  ----------------------------------------------------------------------\n"," |  Data and other attributes defined here:\n"," |\n"," |  __abstractmethods__ = frozenset()\n"," |\n"," |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n"," |\n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from ForestClassifier:\n"," |\n"," |  __sklearn_tags__(self)\n"," |\n"," |  predict(self, X)\n"," |      Predict class for X.\n"," |\n"," |      The predicted class of an input sample is a vote by the trees in\n"," |      the forest, weighted by their probability estimates. That is,\n"," |      the predicted class is the one with highest mean probability\n"," |      estimate across the trees.\n"," |\n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The input samples. Internally, its dtype will be converted to\n"," |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n"," |          converted into a sparse ``csr_matrix``.\n"," |\n"," |      Returns\n"," |      -------\n"," |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n"," |          The predicted classes.\n"," |\n"," |  predict_log_proba(self, X)\n"," |      Predict class log-probabilities for X.\n"," |\n"," |      The predicted class log-probabilities of an input sample is computed as\n"," |      the log of the mean predicted class probabilities of the trees in the\n"," |      forest.\n"," |\n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The input samples. Internally, its dtype will be converted to\n"," |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n"," |          converted into a sparse ``csr_matrix``.\n"," |\n"," |      Returns\n"," |      -------\n"," |      p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n"," |          The class probabilities of the input samples. The order of the\n"," |          classes corresponds to that in the attribute :term:`classes_`.\n"," |\n"," |  predict_proba(self, X)\n"," |      Predict class probabilities for X.\n"," |\n"," |      The predicted class probabilities of an input sample are computed as\n"," |      the mean predicted class probabilities of the trees in the forest.\n"," |      The class probability of a single tree is the fraction of samples of\n"," |      the same class in a leaf.\n"," |\n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The input samples. Internally, its dtype will be converted to\n"," |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n"," |          converted into a sparse ``csr_matrix``.\n"," |\n"," |      Returns\n"," |      -------\n"," |      p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n"," |          The class probabilities of the input samples. The order of the\n"," |          classes corresponds to that in the attribute :term:`classes_`.\n"," |\n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.base.ClassifierMixin:\n"," |\n"," |  score(self, X, y, sample_weight=None)\n"," |      Return the mean accuracy on the given test data and labels.\n"," |\n"," |      In multi-label classification, this is the subset accuracy\n"," |      which is a harsh metric since you require for each sample that\n"," |      each label set be correctly predicted.\n"," |\n"," |      Parameters\n"," |      ----------\n"," |      X : array-like of shape (n_samples, n_features)\n"," |          Test samples.\n"," |\n"," |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n"," |          True labels for `X`.\n"," |\n"," |      sample_weight : array-like of shape (n_samples,), default=None\n"," |          Sample weights.\n"," |\n"," |      Returns\n"," |      -------\n"," |      score : float\n"," |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n"," |\n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n"," |\n"," |  __dict__\n"," |      dictionary for instance variables\n"," |\n"," |  __weakref__\n"," |      list of weak references to the object\n"," |\n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from BaseForest:\n"," |\n"," |  apply(self, X)\n"," |      Apply trees in the forest to X, return leaf indices.\n"," |\n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The input samples. Internally, its dtype will be converted to\n"," |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n"," |          converted into a sparse ``csr_matrix``.\n"," |\n"," |      Returns\n"," |      -------\n"," |      X_leaves : ndarray of shape (n_samples, n_estimators)\n"," |          For each datapoint x in X and for each tree in the forest,\n"," |          return the index of the leaf x ends up in.\n"," |\n"," |  decision_path(self, X)\n"," |      Return the decision path in the forest.\n"," |\n"," |      .. versionadded:: 0.18\n"," |\n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The input samples. Internally, its dtype will be converted to\n"," |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n"," |          converted into a sparse ``csr_matrix``.\n"," |\n"," |      Returns\n"," |      -------\n"," |      indicator : sparse matrix of shape (n_samples, n_nodes)\n"," |          Return a node indicator matrix where non zero elements indicates\n"," |          that the samples goes through the nodes. The matrix is of CSR\n"," |          format.\n"," |\n"," |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n"," |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n"," |          gives the indicator value for the i-th estimator.\n"," |\n"," |  fit(self, X, y, sample_weight=None)\n"," |      Build a forest of trees from the training set (X, y).\n"," |\n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The training input samples. Internally, its dtype will be converted\n"," |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n"," |          converted into a sparse ``csc_matrix``.\n"," |\n"," |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n"," |          The target values (class labels in classification, real numbers in\n"," |          regression).\n"," |\n"," |      sample_weight : array-like of shape (n_samples,), default=None\n"," |          Sample weights. If None, then samples are equally weighted. Splits\n"," |          that would create child nodes with net zero or negative weight are\n"," |          ignored while searching for a split in each node. In the case of\n"," |          classification, splits are also ignored if they would result in any\n"," |          single class carrying a negative weight in either child node.\n"," |\n"," |      Returns\n"," |      -------\n"," |      self : object\n"," |          Fitted estimator.\n"," |\n"," |  ----------------------------------------------------------------------\n"," |  Readonly properties inherited from BaseForest:\n"," |\n"," |  estimators_samples_\n"," |      The subset of drawn samples for each base estimator.\n"," |\n"," |      Returns a dynamically generated list of indices identifying\n"," |      the samples used for fitting each member of the ensemble, i.e.,\n"," |      the in-bag samples.\n"," |\n"," |      Note: the list is re-created at each call to the property in order\n"," |      to reduce the object memory footprint by not storing the sampling\n"," |      data. Thus fetching the property may be slower than expected.\n"," |\n"," |  feature_importances_\n"," |      The impurity-based feature importances.\n"," |\n"," |      The higher, the more important the feature.\n"," |      The importance of a feature is computed as the (normalized)\n"," |      total reduction of the criterion brought by that feature.  It is also\n"," |      known as the Gini importance.\n"," |\n"," |      Warning: impurity-based feature importances can be misleading for\n"," |      high cardinality features (many unique values). See\n"," |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n"," |\n"," |      Returns\n"," |      -------\n"," |      feature_importances_ : ndarray of shape (n_features,)\n"," |          The values of this array sum to 1, unless all trees are single node\n"," |          trees consisting of only the root node, in which case it will be an\n"," |          array of zeros.\n"," |\n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n"," |\n"," |  __getitem__(self, index)\n"," |      Return the index'th estimator in the ensemble.\n"," |\n"," |  __iter__(self)\n"," |      Return iterator over estimators in the ensemble.\n"," |\n"," |  __len__(self)\n"," |      Return the number of estimators in the ensemble.\n"," |\n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.base.BaseEstimator:\n"," |\n"," |  __getstate__(self)\n"," |      Helper for pickle.\n"," |\n"," |  __repr__(self, N_CHAR_MAX=700)\n"," |      Return repr(self).\n"," |\n"," |  __setstate__(self, state)\n"," |\n"," |  __sklearn_clone__(self)\n"," |\n"," |  get_params(self, deep=True)\n"," |      Get parameters for this estimator.\n"," |\n"," |      Parameters\n"," |      ----------\n"," |      deep : bool, default=True\n"," |          If True, will return the parameters for this estimator and\n"," |          contained subobjects that are estimators.\n"," |\n"," |      Returns\n"," |      -------\n"," |      params : dict\n"," |          Parameter names mapped to their values.\n"," |\n"," |  set_params(self, **params)\n"," |      Set the parameters of this estimator.\n"," |\n"," |      The method works on simple estimators as well as on nested objects\n"," |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n"," |      parameters of the form ``<component>__<parameter>`` so that it's\n"," |      possible to update each component of a nested object.\n"," |\n"," |      Parameters\n"," |      ----------\n"," |      **params : dict\n"," |          Estimator parameters.\n"," |\n"," |      Returns\n"," |      -------\n"," |      self : estimator instance\n"," |          Estimator instance.\n"," |\n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n"," |\n"," |  get_metadata_routing(self)\n"," |      Get metadata routing of this object.\n"," |\n"," |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n"," |      mechanism works.\n"," |\n"," |      Returns\n"," |      -------\n"," |      routing : MetadataRequest\n"," |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n"," |          routing information.\n"," |\n"," |  ----------------------------------------------------------------------\n"," |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n"," |\n"," |  __init_subclass__(**kwargs)\n"," |      Set the ``set_{method}_request`` methods.\n"," |\n"," |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n"," |      looks for the information available in the set default values which are\n"," |      set using ``__metadata_request__*`` class attributes, or inferred\n"," |      from method signatures.\n"," |\n"," |      The ``__metadata_request__*`` class attributes are used when a method\n"," |      does not explicitly accept a metadata through its arguments or if the\n"," |      developer would like to specify a request value for those metadata\n"," |      which are different from the default ``None``.\n"," |\n"," |      References\n"," |      ----------\n"," |      .. [1] https://www.python.org/dev/peps/pep-0487\n","\n"]}]},{"cell_type":"code","source":["df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"1FgMBEeEukPU","executionInfo":{"status":"ok","timestamp":1759120083157,"user_tz":-330,"elapsed":91,"user":{"displayName":"Dr. Nagendra","userId":"05632517937859890145"}},"outputId":"f20dbdec-6207-42f3-e170-2482e269c6df"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Age  Gender  Height  Weight   BMI          Label\n","0   25       1     175      80  25.3  Normal Weight\n","1   30       0     160      60  22.5  Normal Weight\n","2   35       1     180      90  27.3     Overweight\n","3   40       0     150      50  20.0    Underweight\n","4   45       1     190     100  31.2          Obese"],"text/html":["\n","  <div id=\"df-2943148f-fb35-461b-b58d-2d630a188d24\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Age</th>\n","      <th>Gender</th>\n","      <th>Height</th>\n","      <th>Weight</th>\n","      <th>BMI</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>25</td>\n","      <td>1</td>\n","      <td>175</td>\n","      <td>80</td>\n","      <td>25.3</td>\n","      <td>Normal Weight</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>30</td>\n","      <td>0</td>\n","      <td>160</td>\n","      <td>60</td>\n","      <td>22.5</td>\n","      <td>Normal Weight</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>35</td>\n","      <td>1</td>\n","      <td>180</td>\n","      <td>90</td>\n","      <td>27.3</td>\n","      <td>Overweight</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>40</td>\n","      <td>0</td>\n","      <td>150</td>\n","      <td>50</td>\n","      <td>20.0</td>\n","      <td>Underweight</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>45</td>\n","      <td>1</td>\n","      <td>190</td>\n","      <td>100</td>\n","      <td>31.2</td>\n","      <td>Obese</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2943148f-fb35-461b-b58d-2d630a188d24')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-2943148f-fb35-461b-b58d-2d630a188d24 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-2943148f-fb35-461b-b58d-2d630a188d24');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-fd3e28e8-37bb-4fc4-83c5-162c4c4f5ec1\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fd3e28e8-37bb-4fc4-83c5-162c4c4f5ec1')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-fd3e28e8-37bb-4fc4-83c5-162c4c4f5ec1 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df","summary":"{\n  \"name\": \"df\",\n  \"rows\": 108,\n  \"fields\": [\n    {\n      \"column\": \"Age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 24,\n        \"min\": 11,\n        \"max\": 112,\n        \"num_unique_values\": 75,\n        \"samples\": [\n          45,\n          41,\n          18\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Gender\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Height\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 27,\n        \"min\": 120,\n        \"max\": 210,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          210,\n          160\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Weight\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28,\n        \"min\": 10,\n        \"max\": 120,\n        \"num_unique_values\": 23,\n        \"samples\": [\n          105,\n          20\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BMI\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.5838181808419725,\n        \"min\": 3.9,\n        \"max\": 37.2,\n        \"num_unique_values\": 25,\n        \"samples\": [\n          37.2,\n          28.9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Overweight\",\n          \"Obese\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["65/pow(1.77,2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g9vAt84-unwo","executionInfo":{"status":"ok","timestamp":1759120097445,"user_tz":-330,"elapsed":37,"user":{"displayName":"Dr. Nagendra","userId":"05632517937859890145"}},"outputId":"f586a1e7-33fa-4665-c49f-cc9a7617b796"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["20.747550193111813"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["model.predict([[33,1,177,65,21.1]])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E1zDiHu9upZY","executionInfo":{"status":"ok","timestamp":1759120103930,"user_tz":-330,"elapsed":30,"user":{"displayName":"Dr. Nagendra","userId":"05632517937859890145"}},"outputId":"5fcec5f4-b067-4b91-d19c-e5ff52001004"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["array(['Normal Weight'], dtype=object)"]},"metadata":{},"execution_count":12}]}]}